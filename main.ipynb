{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Modal Defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import pad_sequences\n",
    "import statistics\n",
    "\n",
    "\n",
    "_docEvents = 'mousedown mouseup mousemove mouseover mouseout mousewheel wheel'\n",
    "_docEvents += ' touchstart touchend touchmove deviceorientation keydown keyup keypress'\n",
    "_docEvents += ' click dblclick scroll change select submit reset contextmenu cut copy paste'\n",
    "_winEvents = 'load unload beforeunload blur focus resize error abort online offline'\n",
    "_winEvents += ' storage popstate hashchange pagehide pageshow message beforeprint afterprint'\n",
    "\n",
    "events = _docEvents.split() + _winEvents.split()\n",
    "\n",
    "labels = {\n",
    "        'human': 0,\n",
    "        'gremlins': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Preprocessed Data\n",
    "import pickle\n",
    "pos_dict = {}\n",
    "\n",
    "with open('data/pos_dict.pickle', 'rb') as handle:\n",
    "    pos_dict = pickle.load(handle)\n",
    "\n",
    "\n",
    "# Uni-Modality Version\n",
    "\n",
    "# single_modality_pos_dict = {}\n",
    "# mouse_indices = {}\n",
    "\n",
    "# for c, values in pos_dict['event'].items():\n",
    "#     mouse_indices[c] = []\n",
    "#     for i, v in enumerate(values):\n",
    "#         if v == 2:\n",
    "#             mouse_indices[c].append(i)\n",
    "# print(len(mouse_indices))\n",
    "\n",
    "# for attr, v in pos_dict.items():\n",
    "#     single_modality_pos_dict[attr] = {}  \n",
    "#     for c, values in v.items():\n",
    "#         single_modality_pos_dict[attr][c] = []\n",
    "#         for i in mouse_indices[c]:\n",
    "#             single_modality_pos_dict[attr][c].append(values[i])\n",
    "\n",
    "# pos_dict = single_modality_pos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_dict.keys())\n",
    "for k in list(labels.keys()):\n",
    "    print(k, len(pos_dict['x'][k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate one_hot data\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "\n",
    "def get_timestamp_ms(t):\n",
    "    return time.mktime(t.timetuple()) * 1000\n",
    "\n",
    "n = len(pos_dict['x'])\n",
    "n_x=0\n",
    "n_y=0\n",
    "n_action=0\n",
    "pos_dict['x_one_hot']={}\n",
    "pos_dict['y_one_hot']={}\n",
    "pos_dict['action_encoded']={}\n",
    "pos_dict['action_one_hot']={}\n",
    "pos_dict['time_diff']={}\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "all_unique_labels = set()\n",
    "for k, label in labels.items():\n",
    "    all_unique_labels.update(pos_dict['event'][k])\n",
    "encoder.fit(list(all_unique_labels))\n",
    "\n",
    "for k, label in labels.items():\n",
    "    pos_dict['action_encoded'][k]=encoder.transform(pos_dict['event'][k])\n",
    "\n",
    "for k, label in labels.items():\n",
    "    n_x=np.max([n_x,len(set(pos_dict['dir_X'][k]))])\n",
    "    n_y=np.max([n_y,len(set(pos_dict['dir_Y'][k]))])\n",
    "\n",
    "n_action = len(all_unique_labels)\n",
    "\n",
    "\n",
    "for k, label in labels.items():\n",
    "    pos_dict['x_one_hot'][k] = to_categorical(pos_dict['dir_X'][k], num_classes=n_x)\n",
    "    pos_dict['y_one_hot'][k] = to_categorical(pos_dict['dir_Y'][k], num_classes=n_y)\n",
    "    pos_dict['action_one_hot'][k] = to_categorical(pos_dict['action_encoded'][k]-1, num_classes=n_action)\n",
    "\n",
    "    \n",
    "for k, label in labels.items():\n",
    "    pos_dict['time_diff'][k]=[]\n",
    "    for i in range(len(pos_dict['action'][k])-1):\n",
    "#         if i%10000== 0:\n",
    "#             print('{}/{}'.format(i, len(pos_dict['action'][k])))\n",
    "        pos_dict['time_diff'][k]=np.append(pos_dict['time_diff'][k],get_timestamp_ms(pos_dict['t'][k][i+1])-get_timestamp_ms(pos_dict['t'][k][i]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "mousemove_index = 2\n",
    "\n",
    "def get_train_data(time_steps, data_size=5000, train_index_limit=15000, multiModal=True):\n",
    "    X, y, T = list(), list(), list()\n",
    "    es = []\n",
    "    for k, label in labels.items():\n",
    "        n = train_index_limit if len(pos_dict['x'][k]) > train_index_limit else len(pos_dict['x'][k])\n",
    "        counter=0\n",
    "        while counter < data_size:\n",
    "            x_seq = []\n",
    "            counter+=1\n",
    "            seq_start_i = random.randint(0, n - time_steps - 1)\n",
    "            if multiModal:\n",
    "                x_seq = [\n",
    "                    np.concatenate([pos_dict['action_one_hot'][k][seq_i], pos_dict['x_one_hot'][k][seq_i],\\\n",
    "                                    pos_dict['y_one_hot'][k][seq_i],[pos_dict['time_diff'][k][seq_i]]])\n",
    "                    for seq_i in range(seq_start_i, seq_start_i + time_steps)\n",
    "                ]\n",
    "            else:\n",
    "                seq_i = seq_start_i\n",
    "                while len(x_seq) < time_steps:\n",
    "                    if pos_dict['event'][k][seq_i] == mousemove_index:\n",
    "                        concat = np.concatenate([pos_dict['action_one_hot'][k][seq_i], pos_dict['x_one_hot'][k][seq_i],\\\n",
    "                                    pos_dict['y_one_hot'][k][seq_i],[pos_dict['time_diff'][k][seq_i]]])\n",
    "                        x_seq.append(concat)\n",
    "                    seq_i += 1\n",
    "                    \n",
    "            X.append(np.array(x_seq))\n",
    "            y.append(to_categorical(label, len(set(labels.values()))))        \n",
    "        \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    print(X.shape, y.shape)\n",
    "    X, y = shuffle(X, y, random_state=0)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def get_test_data(time_steps, time_window_ms, train_index_limit=15000, test_size=200, multiModal=True):\n",
    "    X, y, T = list(), list(), list()\n",
    "    es = []\n",
    "\n",
    "    for k, label in labels.items():\n",
    "        \n",
    "        n = len(pos_dict['x'][k])\n",
    "        try_counter = 0\n",
    "        start_index = train_index_limit if len(pos_dict['x'][k]) > train_index_limit else int(len(pos_dict['x'][k])*0.7)\n",
    "\n",
    "        for i in range(test_size):\n",
    "            diff=0\n",
    "            stop_flag = 0\n",
    "            human_counter = 0\n",
    "            while diff < time_window_ms:\n",
    "                \n",
    "                try_counter += 1\n",
    "                stop_flag +=1 \n",
    "                if stop_flag == 100:\n",
    "                    break\n",
    "                seq_start_i = random.randint(start_index, n-100)\n",
    "                x_seq = list()\n",
    "                for seq_i in range(seq_start_i, len(pos_dict['x'][k])-1):\n",
    "                    if seq_start_i > n:\n",
    "                        break\n",
    "                    if seq_i - seq_start_i == time_steps:\n",
    "                        break\n",
    "                        \n",
    "                    human_counter += 1\n",
    "\n",
    "                    diff = pos_dict['t'][k][seq_i] - pos_dict['t'][k][seq_start_i]\n",
    "                    diff = int(diff.total_seconds() * 1000.0)\n",
    "                    if diff > time_window_ms:\n",
    "                        diff = pos_dict['t'][k][seq_i-1] - pos_dict['t'][k][seq_start_i]\n",
    "                        diff = int(diff.total_seconds() * 1000.0)\n",
    "                        break\n",
    "                    \n",
    "                    x_seq.append(np.concatenate([pos_dict['action_one_hot'][k][seq_i], pos_dict['x_one_hot'][k][seq_i],pos_dict['y_one_hot'][k][seq_i],[pos_dict['time_diff'][k][seq_i]]]))\n",
    "            \n",
    "            X.append(x_seq)\n",
    "            y.append(to_categorical(label, len(set(labels.values()))))\n",
    "            T.append(diff)\n",
    "    X, y, T = np.array(X), np.array(y), np.array(T)\n",
    "    print(X.shape, y.shape, T.shape)\n",
    "    X, y, T = shuffle(X, y, T, random_state=0)\n",
    "\n",
    "    return X, y, T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, GRU, Dropout, Bidirectional\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "# n_features = len(events)+3\n",
    "n_features = n_x+n_y+n_action+1\n",
    "\n",
    "print(n_features)\n",
    "# n_features = 3\n",
    "n_steps = 50\n",
    "\n",
    "\n",
    "def train(n_steps, epochs=100, batch_size=100):\n",
    "    print('Train with Size:', n_steps)\n",
    "    X, y = get_train_data(n_steps)\n",
    "\n",
    "    print(X.shape, y.shape)\n",
    "    X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    return_sequences=True\n",
    "\n",
    "    model.add(LSTM(300, return_sequences=return_sequences, activation='tanh', input_shape=(None, n_features)))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(LSTM(200, return_sequences=return_sequences, activation='tanh'))\n",
    "    model.add(LSTM(100, activation='tanh'))\n",
    "    model.add(Dense(len(set(labels.values())), activation='sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    return model\n",
    "\n",
    "def test(model, assurance_threshold=8, test_size=200, train_index_limit=15000):\n",
    "    print('TEST ---------- Size:', n_steps)\n",
    "    time_to_detect = {}\n",
    "    confusion_matrix = {}\n",
    "    n_labels = len(list(labels.values()))\n",
    "    for k, label in labels.items():\n",
    "        n = len(pos_dict['x'][k])\n",
    "        start_index = train_index_limit if len(pos_dict['x'][k]) > train_index_limit else int(len(pos_dict['x'][k])*0.7)\n",
    "        \n",
    "        time_to_detect[k] = []\n",
    "        confusion_matrix[k] = np.zeros((n_labels, n_labels))\n",
    "        for i in range(test_size):\n",
    "            if i%(test_size/2) == 0:\n",
    "                print(k, f\"{i}/{test_size}\")\n",
    "            queue = deque([None] * assurance_threshold)\n",
    "            # print(n, n_steps)\n",
    "            \n",
    "            seq_start_i = random.randint(start_index, n-100)\n",
    "            \n",
    "            # diff = pos_dict['t'][k][seq_start_i+80] - pos_dict['t'][k][seq_start_i]\n",
    "            # diff = int(diff.total_seconds() * 1000.0)\n",
    "            # while diff > 100000:\n",
    "            #     seq_start_i = random.randint(start_index, n-100)\n",
    "            #     diff = pos_dict['t'][k][seq_start_i+100] - pos_dict['t'][k][seq_start_i]\n",
    "            #     diff = int(diff.total_seconds() * 1000.0)\n",
    "            \n",
    "            x_seq = list()\n",
    "            \n",
    "            for seq_i in range(seq_start_i, len(pos_dict['x'][k])-1):\n",
    "                if seq_start_i > n:\n",
    "                    break\n",
    "                if seq_i - seq_start_i == 200:\n",
    "                    break\n",
    "\n",
    "                x_seq.append(np.concatenate([pos_dict['action_one_hot'][k][seq_i], pos_dict['x_one_hot'][k][seq_i],pos_dict['y_one_hot'][k][seq_i],[pos_dict['time_diff'][k][seq_i]]]))                \n",
    "                y_predict = model.predict(np.array(x_seq).reshape((1, np.array(x_seq).shape[0], np.array(x_seq).shape[1])), verbose=0)\n",
    "#                     true_label = to_categorical(label, len(set(labels.values())))\n",
    "                \n",
    "                true_label = label\n",
    "                pred_label = np.argmax(y_predict[0])\n",
    "        \n",
    "                queue.popleft()\n",
    "                queue.append(pred_label)\n",
    "                \n",
    "                if all(i == list(queue)[0] for i in list(queue)):\n",
    "                    pred_label =  list(queue)[0]\n",
    "                    diff = pos_dict['t'][k][seq_i] - pos_dict['t'][k][seq_start_i]\n",
    "                    diff = int(diff.total_seconds() * 1000.0)\n",
    "                    \n",
    "                    confusion_matrix[k][true_label][pred_label] += 1\n",
    "                    \n",
    "                    correct = 0\n",
    "                    if pred_label == true_label:\n",
    "                        correct = 1\n",
    "                    time_to_detect[k].append((diff, seq_i-seq_start_i, correct))\n",
    "                    break\n",
    "\n",
    "                \n",
    "#     print(time_to_detect)\n",
    "                    \n",
    "    return time_to_detect, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "res = {}\n",
    "steps = [40, 50, 60]\n",
    "models = {}\n",
    "epochs = [50]\n",
    "for step in steps:\n",
    "    res[step] = {}\n",
    "    models[step] = {}\n",
    "    for epoch in epochs:\n",
    "        model = train(step, epoch)\n",
    "        # model.save()\n",
    "        models[step][epoch] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "results = {}\n",
    "ts = list(range(2, 20))\n",
    "confusions = {}\n",
    "for t in ts:\n",
    "    res, confusion = test(model, assurance_threshold=t, test_size=200)\n",
    "    confusions[t] = confusion\n",
    "    results[t] = copy.deepcopy(res)\n",
    "    print('------- Threshold', t)\n",
    "    accs = []\n",
    "    ttds = []\n",
    "    etds = []\n",
    "    for label, triples in res.items():\n",
    "        acc = sum([i[2] for i in triples])/len(triples)\n",
    "        accs.append(acc)\n",
    "        ttd = sum([i[0] for i in triples])/len(triples)\n",
    "        ttds.append(ttd)\n",
    "        etd = sum([i[1] for i in triples])/len(triples)\n",
    "        etds.append(etd)\n",
    "        if label == 'random_mouse_bot':\n",
    "            filler = '\\t'\n",
    "        else:\n",
    "            filler = '\\t\\t'\n",
    "        print(label, filler, f'ACC: {acc} \\t TTD: {ttd} \\t ETD: {etd}')\n",
    "    \n",
    "    acc_avg = round(sum(accs)/len(accs), 2)\n",
    "    ttd_avg = round(sum(ttds)/len(ttds), 2)\n",
    "    etd_avg = round(sum(etds)/len(etds), 2)\n",
    "\n",
    "    print(f'\\t \\t \\t acc_avg: {acc_avg}  \\t ttd_avg: {ttd_avg} \\t etd_avg: {etd_avg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute Force Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = n_x+n_y+n_action+1\n",
    "n_steps = 10 # Sequence length\n",
    "mousemove_index = 2\n",
    "train_X, train_y = get_train_data(n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use PyTorch for easier attack implementation\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "\n",
    "epochs=20\n",
    "batch_size=100\n",
    "device = 'cuda:0'\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, n_features, n_classes, return_sequences=True):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.return_sequences = return_sequences\n",
    "\n",
    "        self.lstm1 = nn.LSTM(n_features, 300, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(300, 200, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(200, 100, batch_first=True)\n",
    "\n",
    "        # The final layer's size must be equal to the number of classes\n",
    "        self.fc = nn.Linear(100, n_classes)\n",
    "        \n",
    "        # Sigmoid activation for binary classification or softmax for multiclass\n",
    "        # self.activation = nn.Sigmoid() if n_classes == 2 else nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Assuming x is of shape (batch, seq, feature)\n",
    "        x, _ = self.lstm1(x)\n",
    "\n",
    "        if self.return_sequences:\n",
    "            x, _ = self.lstm2(x)\n",
    "        else:\n",
    "            # Only pass the last output to the next LSTM if not return_sequences\n",
    "            x, (h_n, c_n) = self.lstm2(x)\n",
    "            x = h_n[-1]\n",
    "            \n",
    "        x, _ = self.lstm3(x)\n",
    "\n",
    "        # If return_sequences is True, we need to only take the output at the last timestep\n",
    "        if self.return_sequences:\n",
    "            x = x[:, -1, :]\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        # x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "# Model instantiation\n",
    "n_features = train_X.shape[2]  # Assuming X is (batch, seq, feature)\n",
    "n_classes = train_y.shape[1]\n",
    "\n",
    "model = LSTMModel(n_features=n_features, n_classes=n_classes, return_sequences=False).to(device)\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "train_X_tensor = torch.tensor(train_X, dtype=torch.float32,device=device)\n",
    "train_y_tensor = torch.tensor(train_y, dtype=torch.long,device=device)\n",
    "\n",
    "\n",
    "\n",
    "if len(train_y_tensor.shape) > 1 and train_y_tensor.shape[1] > 1:\n",
    "    _, train_y_tensor = train_y_tensor.max(dim=1)\n",
    "\n",
    "# Create a dataset and dataloader for batching\n",
    "dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch model train\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # It applies softmax internally\n",
    "optimizer = Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Initialize loss for the epoch\n",
    "    epoch_loss = 0.0\n",
    "    # Loop over the data in batches using the dataloader\n",
    "    for batch_x, batch_y in tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "        optimizer.zero_grad()  # Clear gradients for the next train\n",
    "        output = model(batch_x)  # Forward pass\n",
    "        loss = criterion(output, batch_y)  # Calculate loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update the weights\n",
    "        epoch_loss += loss.item() * batch_x.size(0)  # Multiply by batch size since loss is averaged\n",
    "\n",
    "    # Calculate the average loss for this epoch\n",
    "    epoch_loss /= len(dataloader.dataset)\n",
    "    train_losses.append(epoch_loss)  # Append the average loss for the epoch to the list\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "plt.figure(figsize=(10, 5))  \n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.title('Train Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def test(model, assurance_threshold=8, test_size=200, train_index_limit=15000):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_set = []\n",
    "    test_set_whole = []\n",
    "    with torch.no_grad():  # Inference without tracking gradients\n",
    "        print('TEST ----------')\n",
    "        time_to_detect = {}\n",
    "        confusion_matrix = {}\n",
    "        n_labels = len(set(labels.values()))\n",
    "\n",
    "        for k, label in labels.items():\n",
    "            n = len(pos_dict['x'][k])\n",
    "            start_index = train_index_limit if len(pos_dict['x'][k]) > train_index_limit else int(n * 0.7)\n",
    "            \n",
    "            time_to_detect[k] = []\n",
    "            confusion_matrix[k] = np.zeros((n_labels, n_labels))\n",
    "\n",
    "            for i in range(test_size):\n",
    "                if i % (test_size // 2) == 0:\n",
    "                    print(k, f\"{i}/{test_size}\")\n",
    "                queue = deque([None] * assurance_threshold)\n",
    "                \n",
    "                seq_start_i = random.randint(start_index, n - 100)\n",
    "                \n",
    "                x_seq = []\n",
    "\n",
    "                for seq_i in range(seq_start_i, min(n - 1, seq_start_i + 200)):\n",
    "                    x_seq.append(np.concatenate([\n",
    "                        pos_dict['action_one_hot'][k][seq_i],\n",
    "                        pos_dict['x_one_hot'][k][seq_i],\n",
    "                        pos_dict['y_one_hot'][k][seq_i],\n",
    "                        [pos_dict['time_diff'][k][seq_i]]\n",
    "                    ]))\n",
    "\n",
    "                    input_tensor = torch.tensor(np.array([x_seq]), dtype=torch.float32).to(device)  # Shape (1, seq_len, features)\n",
    "                    y_predict = model(input_tensor).cpu().numpy()  # Inference and convert to numpy array\n",
    "                    pred_label = np.argmax(y_predict, axis=1)[0]  # Get the predicted label\n",
    "                    test_set.append((input_tensor,pred_label,label))\n",
    "                    queue.popleft()\n",
    "                    queue.append(pred_label)\n",
    "                    \n",
    "                    if all(v == queue[0] for v in queue):\n",
    "                        pred_label = queue[0]\n",
    "                        diff = (pos_dict['t'][k][seq_i] - pos_dict['t'][k][seq_start_i]).total_seconds() * 1000.0\n",
    "                        \n",
    "                        confusion_matrix[k][label][pred_label] += 1\n",
    "                        \n",
    "                        correct = int(pred_label == label)\n",
    "                        time_to_detect[k].append((int(diff), seq_i - seq_start_i, correct))\n",
    "                        break\n",
    "                \n",
    "                x = torch.tensor(np.array([x_seq]), dtype=torch.float32).to(device)\n",
    "                y_pred = model(x).cpu().numpy()  # Inference and convert to numpy array\n",
    "                y_label = np.argmax(y_predict, axis=1)[0]  # Get the predicted label\n",
    "                test_set_whole.append((x,y_label,label))\n",
    "\n",
    "        return time_to_detect, confusion_matrix, test_set, test_set_whole\n",
    "\n",
    "import copy\n",
    "\n",
    "results = {}\n",
    "# ts = list(range(2, 20,4))\n",
    "ts = [10]\n",
    "confusions = {}\n",
    "acc_avgs = []\n",
    "test_sets = []\n",
    "for t in ts:\n",
    "    res, confusion,test_set, test_set_whole = test(model, assurance_threshold=t, test_size=200)\n",
    "    test_sets.append(test_set_whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_ = test_sets[0] # diffferent lengths\n",
    "print(len(test_set_))\n",
    "\n",
    "data_label = [[] for _ in labels.keys()]\n",
    "for data in test_set_:\n",
    "    x,p_y,y = data\n",
    "    data_label[y].append((x,p_y))\n",
    "\n",
    "index = 5 # Random index to check dimensions of samples\n",
    "label = 1\n",
    "\n",
    "input_sequence = data_label[label][index][0] # get the actual data not the label (2nd item)\n",
    "# input_sequence = torch.split(input_sequence, 5, dim=1)[3]\n",
    "print(input_sequence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute Force Attack based on the test samples\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "def brute_force(min_time, max_time, source_label=1, seq_len=3, test_index=0):\n",
    "    print('Source = ', source_label)\n",
    "    test_set_ = test_sets[0] # diffferent lengths\n",
    "#     print(len(test_set_))\n",
    "\n",
    "    data_label = [[] for _ in labels.keys()]\n",
    "    for data in test_set_:\n",
    "        x,p_y,y = data\n",
    "        data_label[y].append((x,p_y))\n",
    "\n",
    "    index = test_index\n",
    "    \n",
    "    label = source_label\n",
    "\n",
    "        \n",
    "    if data_label[label][index][0].shape[1] < seq_len:\n",
    "        print('No seq long enough...')\n",
    "        return \n",
    "#     print(data_label[label][index])\n",
    "    input_sequence = data_label[label][index][0] # get the actual data not the label (2nd item)\n",
    "    input_sequence = torch.split(input_sequence, seq_len, dim=1)[0]\n",
    "    print(input_sequence.shape)\n",
    "    \n",
    "    extensions = []\n",
    "    seq_len = seq_len\n",
    "    count = 0 \n",
    "    for c1 in itertools.combinations_with_replacement(list(range(5)), seq_len):\n",
    "        for c2 in itertools.combinations_with_replacement(list(range(5)), seq_len):\n",
    "            for t in itertools.combinations_with_replacement(list(range(min_time, max_time)), seq_len):\n",
    "                count += 1\n",
    "                extensions.append([c for c in c1] + [c for c in c2] + [c for c in t])\n",
    "\n",
    "    global_index = 0\n",
    "\n",
    "    n_batches = 500\n",
    "\n",
    "    batch_size = int((count-(count%n_batches))/n_batches)\n",
    "\n",
    "\n",
    "    successful_attacks = []\n",
    "    for batch_index in range(n_batches):\n",
    "        input_sequence_batch = input_sequence.repeat(batch_size, 1, 1)\n",
    "    #     print(input_sequence_batch.shape)\n",
    "        if batch_index%100 == 0:\n",
    "            print(f'{batch_index}/{n_batches}')\n",
    "        for batch_index in range(batch_size):\n",
    "            for s in range(seq_len):\n",
    "                input_sequence_batch[batch_index, s, -11:-6] = torch.nn.functional.one_hot(torch.tensor(extensions[global_index][0+s]), num_classes=5)\n",
    "                input_sequence_batch[batch_index, s, -6:-1] = torch.nn.functional.one_hot(torch.tensor(extensions[global_index][seq_len+s]), num_classes=5)\n",
    "                input_sequence_batch[batch_index, s, -1] = torch.tensor((extensions[global_index][2*seq_len+s])).float()\n",
    "\n",
    "        global_index += 1\n",
    "\n",
    "        input_sequence_batch = input_sequence_batch.to(device)\n",
    "        target_label = torch.zeros(batch_size, dtype=torch.long, device=device)\n",
    "        with torch.no_grad():\n",
    "            final_output = model(input_sequence_batch)\n",
    "            final_predictions = final_output.argmax(dim=1)\n",
    "\n",
    "        # Check for successful attacks\n",
    "        attack_success_indices = torch.where(final_predictions == target_label)[0]\n",
    "\n",
    "        # successful_attacks = [(x_vals[i].item(), y_vals[i].item(), time_vals[i].item()) for i in attack_success_indices]\n",
    "        successful_attacks.extend([input_sequence_batch[i] for i in attack_success_indices]) \n",
    "    \n",
    "#     print(count, len(successful_attacks), len(successful_attacks[0]))\n",
    "    return count, successful_attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and Time the attack for differnet settings\n",
    "import time\n",
    "\n",
    "res = {}\n",
    "for t in range(15, 40, 3):\n",
    "    for label in [1]:\n",
    "        if t not in res:\n",
    "            res[t] = {}\n",
    "        \n",
    "        if label not in res[t]:\n",
    "            res[t][label] = {}\n",
    "        for s in [3, 6, 10]:\n",
    "            if s not in res[t][label]:\n",
    "                res[t][label][s] = []\n",
    "            else:\n",
    "                continue\n",
    "            start = time.time()\n",
    "            print('----', t, s)\n",
    "            count, successful_attacks = brute_force(t, t+1, source_label=label, seq_len=s)\n",
    "            end = time.time()\n",
    "            print('---- Time:', end - start)\n",
    "            print(\"===== Res\", count, len(successful_attacks), len(successful_attacks)/count*100)\n",
    "            res[t][label][s] = [count, len(successful_attacks), len(successful_attacks)/count*100]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
